{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17885, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>If two guys are discussing the geofinancial an...</td>\n",
       "      <td>rockets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15323</td>\n",
       "      <td>76ers vs Nuggets\\n\\nNew era.</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5551</td>\n",
       "      <td>u/dr-wong u/sirjackiechiles this hoes body wou...</td>\n",
       "      <td>rockets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8343</td>\n",
       "      <td>Idk if I wanna watch. So disappointed</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6282</td>\n",
       "      <td>now, this is the type of content i like to see...</td>\n",
       "      <td>rockets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body subreddit\n",
       "415    If two guys are discussing the geofinancial an...   rockets\n",
       "15323                       76ers vs Nuggets\\n\\nNew era.       nba\n",
       "5551   u/dr-wong u/sirjackiechiles this hoes body wou...   rockets\n",
       "8343               Idk if I wanna watch. So disappointed       nba\n",
       "6282   now, this is the type of content i like to see...   rockets"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>#MORTAL KOMBAT!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8591</td>\n",
       "      <td>He played fewer mins last season than in 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17452</td>\n",
       "      <td>Double-edged sword, wait until Marcus Smart dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14629</td>\n",
       "      <td>Harden</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16640</td>\n",
       "      <td>Never forget this pearler - https://youtu.be/5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  target\n",
       "7250                                   #MORTAL KOMBAT!!!       1\n",
       "8591       He played fewer mins last season than in 2016       0\n",
       "17452  Double-edged sword, wait until Marcus Smart dr...       0\n",
       "14629                                             Harden       0\n",
       "16640  Never forget this pearler - https://youtu.be/5...       0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'] = df['subreddit'].map({'nba': 0, 'rockets': 1})\n",
    "df.drop('subreddit', axis=1, inplace=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body      0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no na values in dataframe\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1336"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many duplicate comments there are\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>Says the dumbass</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>I had to unfollow Lebron on everything. I thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>I had to unfollow Lebron on everything. I thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  target\n",
       "13                                          [removed]       1\n",
       "41                                   Says the dumbass       1\n",
       "63                                          [removed]       1\n",
       "72   I had to unfollow Lebron on everything. I thi...       1\n",
       "73   I had to unfollow Lebron on everything. I thi...       1\n",
       "83                                          [removed]       1\n",
       "84                                          [removed]       1\n",
       "92                                          [removed]       1\n",
       "96                                          [removed]       1\n",
       "97                                          [removed]       1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of the duplicate comments are comments that were removed \n",
    "# duplicate comments were ether removed comments or comments that were posted twice with same text \n",
    "df[df.duplicated()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows where there are duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16549, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where body = ''\n",
    "df = df[df['body'] != '']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16549, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The change with China is going to happen when its economic bubble eventually bursts.\\n\\nThis is probably the best short breakdown of the Chinese cultural psyche that I've seen online:\\n\\nhttps://www.resetera.com/threads/rockets-gm-daryl-morey-tweets-in-support-of-hong-kong-protest-rockets-organization-denounces-tweet-update-nba-suspended-in-china.145209/page-13#post-25281281\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The change with China is going to happen when its economic bubble eventually bursts.\n",
      "\n",
      "This is probably the best short breakdown of the Chinese cultural psyche that I've seen online:\n",
      "\n",
      "https://www.resetera.com/threads/rockets-gm-daryl-morey-tweets-in-support-of-hong-kong-protest-rockets-organization-denounces-tweet-update-nba-suspended-in-china.145209/page-13#post-25281281\n",
      "\n",
      "The change with China is going to happen when its economic bubble eventually bursts.\n",
      "\n",
      "This is probably the best short breakdown of the Chinese cultural psyche that I've seen online:\n",
      "\n",
      "https://www.resetera.com/threads/rockets-gm-daryl-morey-tweets-in-support-of-hong-kong-protest-rockets-organization-denounces-tweet-update-nba-suspended-in-china.145209/page-13#post-25281281\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "# Initialize the BeautifulSoup object on a single movie review     \n",
    "example1 = BeautifulSoup(df['body'][2])\n",
    "\n",
    "# Print the raw review and then the output of get_text(), for \n",
    "# comparison\n",
    "print(df['body'][2])\n",
    "print()\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text())   # The text to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert letters_only to lower case.\n",
    "lower_case = letters_only.lower()\n",
    "\n",
    "# Split lower_case up at each space.\n",
    "words = lower_case.split() # This is like a manual tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'change',\n",
       " 'with',\n",
       " 'china',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'happen',\n",
       " 'when',\n",
       " 'its']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first ten words.\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion adapted from Lecture \n",
    "\n",
    "def review_to_words(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML.\n",
    "    review_text = BeautifulSoup(text).get_text()\n",
    "\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    text = re.sub(r'\\&\\w*;', '', text)\n",
    "    \n",
    "    # Remove non-letters. (Punctuation)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n",
    "    \n",
    "    # Remove words with 2 or fewer letters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "    \n",
    "    # Remove whitespaces\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16549, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the change with china going happen when its economic bubble eventually bursts this probably the best short breakdown the chinese cultural psyche that seen online https www resetera com threads rockets daryl morey tweets support hong kong protest rockets organization denounces tweet update nba suspended china page post '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    words = text.split()\n",
    "    lemma_words = ''\n",
    "    for word in words:\n",
    "        lemma_words += (lemmatizer.lemmatize(word) + ' ')\n",
    "    return lemma_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16549, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most common words from each subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequently used words, can change max_features\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = \"english\", \n",
    "                             max_features = 35) \n",
    "\n",
    "# input for CountVectorizer is an array of strings\n",
    "vector_input_rockets = df[df['target'] == 1]['body']\n",
    "\n",
    "# fit_transform the vectorizer\n",
    "rockets_words = count_vect.fit_transform(vector_input_rockets)\n",
    "\n",
    "# convert output to a Numpy array\n",
    "rockets_words = rockets_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['china', 'chinese', 'don', 'fan', 'fuck', 'game', 'going', 'good', 'guy', 'ha', 'harden', 'just', 'know', 'like', 'lol', 'look', 'make', 'morey', 'nba', 'people', 'player', 'really', 'right', 'rocket', 'rus', 'say', 'season', 'team', 'thing', 'think', 'time', 'wa', 'want', 'way', 'year']\n"
     ]
    }
   ],
   "source": [
    "# rockets most common words\n",
    "rockets_word_list = count_vect.get_feature_names()\n",
    "print(rockets_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequently used words, can change max_features\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None,\n",
    "                             stop_words = \"english\", \n",
    "                             max_features = 35) \n",
    "\n",
    "# input for CountVectorizer is an array of strings\n",
    "vector_input_nba = df[df['target'] == 0]['body']\n",
    "\n",
    "# fit_transform the vectorizer\n",
    "nba_words = count_vect.fit_transform(vector_input_nba)\n",
    "\n",
    "# convert output to a Numpy array\n",
    "nba_words = nba_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'better', 'china', 'doesn', 'don', 'fan', 'game', 'going', 'good', 'got', 'guy', 'ha', 'just', 'know', 'lebron', 'like', 'lol', 'make', 'nba', 'people', 'play', 'player', 'point', 'really', 'right', 'say', 'season', 'shit', 'team', 'thing', 'think', 'time', 'wa', 'way', 'year']\n"
     ]
    }
   ],
   "source": [
    "# nba most common words\n",
    "nba_word_list = count_vect.get_feature_names()\n",
    "print(nba_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the most common words that are in both subreddits to stopwords 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find elements that are the same in two lists \n",
    "\n",
    "def common_words(a, b): \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "    if (a_set & b_set): \n",
    "        print(a_set & b_set) \n",
    "    else: \n",
    "        print(\"No common elements\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player', 'just', 'game', 'don', 'like', 'thing', 'know', 'right', 'people', 'year', 'season', 'guy', 'say', 'going', 'really', 'think', 'lol', 'time', 'way', 'make', 'team', 'nba', 'ha', 'good', 'china', 'fan', 'wa'}\n"
     ]
    }
   ],
   "source": [
    "common_words(rockets_word_list, nba_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_words = ['game', 'make', 'guy', 'lol', 'going', 'time', 'player', 'season', 'think', 'wa', 'china', 'like', 'team', 'thing', 'year', 'nba', 'don', 'really', 'fan', 'people', 'know', 'say', 'way', 'ha', 'right', 'good', 'just']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending stopwords.words('english') to include words that are common in both subreddits\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(union_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'stopwords' (list)\n"
     ]
    }
   ],
   "source": [
    "%store stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
